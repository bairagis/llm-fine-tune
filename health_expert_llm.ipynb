{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RVhYk5FZ_lb",
        "outputId": "6b061cbb-c80f-407c-cddb-b0c613024936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.55.0)\n",
            "Collecting peft\n",
            "  Using cached peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in ./.venv/lib/python3.11/site-packages (1.10.0)\n",
            "Collecting trl\n",
            "  Using cached trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from peft) (7.0.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.11/site-packages (from peft) (2.8.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.13.0->peft) (80.9.0)\n",
            "Collecting datasets>=3.0.0 (from trl)\n",
            "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=3.0.0->trl)\n",
            "  Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0.0->trl)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets>=3.0.0->trl)\n",
            "  Using cached pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Collecting xxhash (from datasets>=3.0.0->trl)\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Using cached aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets>=3.0.0->trl)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets>=3.0.0->trl)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
            "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, bitsandbytes, peft, datasets, trl\n",
            "\u001b[2K  Attempting uninstall: fsspecm\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [pyarrow]\n",
            "\u001b[2K    Found existing installation: fsspec 2025.7.05;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [pyarrow]\n",
            "\u001b[2K    Uninstalling fsspec-2025.7.0:;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [pyarrow]\n",
            "\u001b[2K      Successfully uninstalled fsspec-2025.7.08;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [pyarrow]\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [trl]237m━\u001b[0m \u001b[32m19/20\u001b[0m [trl]sets]tes]\n",
            "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 bitsandbytes-0.46.1 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.3 multiprocess-0.70.16 pandas-2.3.1 peft-0.17.0 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 trl-0.21.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers peft bitsandbytes accelerate trl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RaNkZNI7b3MF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLHnBie5fgiD",
        "outputId": "5c41d6cd-3ea6-47cc-fb03-e782526261a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "# use the pipeline for question - answer\n",
        "pipeline_qn = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70fBRBFPhnYJ",
        "outputId": "51d1c0e9-845a-4632-90fc-6990dd6e912c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.Your comments are  helpful. I am glad I am not the only one who has experienced this. I am also worried about my future health. I hope I am going to be okay. Please do the same thing and let me know if you feel the same. I feel very confused.'}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use qa_pipeline\n",
        "context = \"You are a doctor\"\n",
        "question = \"I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.Your comments are \"\n",
        "\n",
        "pipeline_qn(text_inputs=question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q28W-TiVjni2"
      },
      "outputs": [],
      "source": [
        "# Example: asking the model to summarize a text\n",
        "text_to_summarize = \"The quick brown fox jumps over the lazy dog. The fox is known for its speed, while the dog is known for its laziness. This simple sentence is often used to demonstrate a variety of linguistic concepts.\"\n",
        "\n",
        "# The prompt must follow the model's expected format\n",
        "prompt = f\"[INST]You are a doctor. Summarize the following text: {question}[/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZskANuejpP2",
        "outputId": "d0b7cc8e-5d47-416c-f143-2baee278c48a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm not sure what to make of your description. It sounds like you're experiencing symptoms of vertigo, which can be caused by a variety of factors, including inner ear problems, medication side effects, or even motion sickness.\n",
            "\n",
            "Vertigo is a condition where you feel like you're spinning or feeling off-balance, even when you're not actually moving around. This can be quite disorienting and uncomfortable. The good news is that vertigo is usually treatable with some simple lifestyle changes, such as avoiding triggers, taking medication, or practicing relaxation techniques.\n",
            "\n",
            "Here are some possible explanations for your symptoms:\n",
            "\n",
            "1. **Inner ear problems**: You may have an inner ear disorder, such as benign paroxysmal positional vertigo (BPPV), labyrinthitis, or vestibular neuritis, which can cause vertigo.\n",
            "2. **Medication side effects**: Certain medications, such as antibiotics, antihistamines, or antidepressants, can cause vertigo as a side effect.\n",
            "3. **Motion sickness**: You may be experiencing motion sickness, which is a common condition that can cause dizziness and nausea.\n",
            "4. **Other medical conditions**: Other conditions, such as high blood pressure, migraines, or Meniere's disease, can also cause vertigo.\n",
            "\n",
            "It's not normal stomach discomfort, and if you're experiencing vertigo, it's essential to consult with a healthcare professional to rule out any underlying medical conditions. They can help determine the cause of your symptoms and provide guidance on the best course of treatment.\n",
            "\n",
            "In the meantime, here are some tips that may help alleviate your symptoms:\n",
            "\n",
            "* **Take it easy**: Avoid strenuous activities and try to rest as much as possible.\n",
            "* **Stay hydrated**: Drink plenty of water to help your body recover.\n",
            "* **Avoid triggers**: If you're experiencing motion sickness, try to avoid triggers such as heavy lifting, bending, or traveling by car.\n",
            "* **Practice relaxation techniques**: Try relaxation techniques, such as deep breathing, meditation, or yoga, to help manage your symptoms.\n",
            "\n",
            "Remember, vertigo is usually a temporary condition, and with some simple lifestyle changes and medical attention, you can manage your symptoms and feel better soon.\n"
          ]
        }
      ],
      "source": [
        "# Run the pipeline with the formatted prompt\n",
        "result = pipeline_qn(prompt, max_new_tokens=500, do_sample=True, temperature=0.7)\n",
        "\n",
        "# The result is a list of dictionaries, so you'll need to extract the generated text\n",
        "generated_text = result[0]['generated_text']\n",
        "\n",
        "# remove all the texts between [INST] and [/INST] from the generated text. Use string index to Find [INST] and [/INST] and remove all the caharacters in between these including [INST] abd [/INST]\n",
        "start_index = generated_text.find(\"[INST]\")\n",
        "end_index = generated_text.find(\"[/INST]\")\n",
        "print(generated_text[end_index+7:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AhFO3QjLowum"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IkBuQbxlpbeK"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "\n",
        "# def format_data(example):\n",
        "#     context = f\"{example['instruction']} {example['input']}\\n doctor: {example['output']}\"\n",
        "#     return tokenizer(context, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# formatted_data = ds.map(format_data, remove_columns=ds[\"train\"].column_names)\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Example formatting function\n",
        "def format_data_for_llama(row):\n",
        "    # This is a common template for Llama instruct models.\n",
        "    # The 'input' field is optional and can be an empty string if not provided.\n",
        "    prompt = f\"[INST] {row['instruction']}\\n{row['input']} [/INST]\"\n",
        "    response = f\"{row['output']}\"\n",
        "    # Combine the prompt and response for the model to learn the full sequence.\n",
        "    return {\"text\": prompt + response}\n",
        "\n",
        "# Apply the formatting to the entire dataset\n",
        "formatted_dataset = ds.map(format_data_for_llama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDZabxoJyKq3",
        "outputId": "2638712d-e242-4018-8d3f-f0c0ad9533d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'text'],\n",
              "    num_rows: 112165\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tfZnZptHr7uN"
      },
      "outputs": [],
      "source": [
        "# Set up quantization config for 4-bit loading (saves GPU memory)\n",
        "# This is crucial for running larger models on limited hardware.\n",
        "# from bitsandbytes import BitsAndBytesConfig\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  \n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
        "    bnb_4bit_use_double_quant=True,  # ✅ Enable double quant for more VRAM savings\n",
        ")\n",
        "\n",
        "\n",
        "# LoRA configuration (Parameter-Efficient Fine-Tuning)\n",
        "# LoRA injects small, trainable matrices into the model, making fine-tuning\n",
        "# much more efficient than updating all model parameters.\n",
        "lora_config = LoraConfig(\n",
        "    r=4, # Rank of the update matrices (adjust based on experimentation)\n",
        "    lora_alpha=16, # Scaling factor for LoRA\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # Modules to apply LoRA to\n",
        "    lora_dropout=0.05, # Dropout probability for LoRA layers\n",
        "    bias=\"none\", # Do not train bias terms\n",
        "    task_type=\"CAUSAL_LM\", # Specify the task type\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "# These control the training process, including learning rate, epochs, etc.\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=1,         # minimal batch\n",
        "    gradient_accumulation_steps=32,        # reduce from 64 if you want faster step time\n",
        "    warmup_steps=2,\n",
        "    max_steps=50,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=200,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    dataloader_num_workers=2,\n",
        "    gradient_checkpointing=True,           # << reduces VRAM usage\n",
        ")\n",
        "\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     per_device_train_batch_size=2,\n",
        "#     gradient_accumulation_steps=4,\n",
        "#     learning_rate=2e-4,\n",
        "#     max_grad_norm=0.3,\n",
        "#     max_steps=500,\n",
        "#     logging_steps=10,\n",
        "#     fp16=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aySPY3tcu8Sr",
        "outputId": "58dc7de3-3d29-47c9-d5f4-e7f713f90b0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'text'],\n",
              "        num_rows: 112165\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reduce the dataset to 1000 samples for faster training\n",
        "formatted_dataset = formatted_dataset['train'].shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'text'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AfbOG5du0l3s"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'meta-llama/Llama-3.2-1B-Instruct',\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Automatically maps model to available devices (GPU/CPU)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size after quantization: 262.74 million parameters\n"
          ]
        }
      ],
      "source": [
        "# what is the model size after quantization\n",
        "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model size after quantization: {model_size / 1e6:.2f} million parameters\")  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "22KDDBIm0dM1"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "KNmoPEy5uqQR",
        "outputId": "fdc1293c-7876-4dd4-cffd-8488bdf20456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing SFTTrainer...\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing SFTTrainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=formatted_dataset,\n",
        "    peft_config=lora_config,\n",
        "    args=training_arguments\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4n45_0HD2iey"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"health_assistant_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "# load the model from the saved directory\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./model/health_assistant_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "# Example usage of the fine-tuned model\n",
        "pipeline_qn = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wrap the above code in a function\n",
        "def ask_health_expert(question,pipeline):    \n",
        "    prompt = f\"[INST]You are a doctor. Generate answer to my question.My question: {question} . Your answer is : [/INST]\"\n",
        "    result = pipeline(prompt, max_new_tokens=500, do_sample=True, temperature=0.7)\n",
        "    generated_text = result[0]['generated_text']\n",
        "    start_index = generated_text.find(\"[INST]\")\n",
        "    end_index = generated_text.find(\"[/INST]\")\n",
        "    generated_txt=generated_text[end_index+7:]  # Return the answer after the [/INST] tag\n",
        "\n",
        "    # Remove any [INST] or [/INST] od \"[\" , \"]\"\n",
        "    generated_txt = generated_txt.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return generated_txt\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-15): 16 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load base model\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # e.g., \"meta-llama/Llama-2-7b-hf\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load PEFT adapter\n",
        "peft_model_id = \"./model/health_assistant_model\"  # path where you saved with save_model()\n",
        "model_peft = PeftModel.from_pretrained(base_model, peft_model_id)\n",
        "\n",
        "# Merge adapter into base model (optional but can improve performance)\n",
        "# model = model.merge_and_unload()\n",
        "\n",
        "# Set to evaluation mode\n",
        "model_peft.eval()\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_peft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipeline_qn_peft = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_peft,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi there, I can see that you are experiencing symptoms of vertigo, which is a sensation of spinning or dizziness\n",
            "Vertigo is a type of balance disorder that can be caused by a variety of factors, including inner ear problems, neurological disorders, or even medication side effects\n",
            "The fact that you are experiencing symptoms of vertigo even after taking Panadol (pantoprazole) and sleeping for a few hours suggests that the issue may not be related to your medication\n",
            "Panadol is an antacid that can help alleviate nausea and vomiting, but it is not a treatment for vertigo\n",
            "Vertigo is typically caused by a problem with the inner ear, such as a vestibular disorder, which can be caused by a variety of factors, including inner ear problems, neurological disorders, or even medication side effects\n",
            "It is also possible that you may be experiencing motion sickness, which is a common cause of vertigo\n",
            "If you are experiencing symptoms of vertigo, I would recommend consulting an ENT specialist or a neurologist to determine the cause and develop a treatment plan to alleviate your symptoms\n",
            "They can also help you determine whether you are experiencing motion sickness or another cause of vertigo.\n"
          ]
        }
      ],
      "source": [
        "question = \"I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.Your comments are \"\n",
        "\n",
        "result = ask_health_expert(question,pipeline_qn_peft)\n",
        "\n",
        "# structure the long text output from the model\n",
        "def structure_output(text):\n",
        "    # Split the text into sentences or paragraphs\n",
        "    sentences = text.split('. ')\n",
        "    structured_output = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        # Clean up each sentence\n",
        "        cleaned_sentence = sentence.strip()\n",
        "        if cleaned_sentence:  # Only add non-empty sentences\n",
        "            structured_output.append(cleaned_sentence)\n",
        "    \n",
        "    return structured_output\n",
        "\n",
        "structured_result_1 = structure_output(result)\n",
        "\n",
        "for idx, sentence in enumerate(structured_result_1, 1):\n",
        "    print(f\"{sentence}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://f1bc8268b8e6091289.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f1bc8268b8e6091289.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# create a gradio interface for the health expert model\n",
        "import gradio as gr \n",
        "def gradio_interface(question):\n",
        "    answer = ask_health_expert(question, pipeline_qn_peft)\n",
        "    structured_answer = structure_output(answer)\n",
        "    return structured_answer\n",
        "\n",
        "gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Textbox(label=\"Ask a health-related question\"),\n",
        "    outputs=gr.Text(label=\"Structured Answer\"),\n",
        "    title=\"Health Expert LLM\",\n",
        "    description=\"Ask a health-related question and get answers from the fine-tuned LLM.\"\n",
        ").launch(share=True)  # Set share=True to create a public link\n",
        "# Note: You can set share=False if you want to run it locally without a public link \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'system',\n",
              "    'content': \"You are a health expert. Answer the user's question in detail.\"},\n",
              "   {'role': 'user',\n",
              "    'content': 'I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.'},\n",
              "   {'role': 'assistant',\n",
              "    'content': \"I can help you try to understand what might be going on. The symptoms you're describing, such as the spinning sensation when you're sitting down, feeling nauseous and vomiting, and the spinning sensation when you're lying down, are common symptoms of a condition called Orthostatic Intolerance, also known as Orthostatic Dizziness or Postural Orthostatic Tachycardia Syndrome (POTS).\\n\\nOrthostatic intolerance is a condition where the body's autonomic nervous system (ANS) fails to maintain blood flow and blood pressure when changing positions, such as from sitting to standing. This can lead to symptoms like dizziness, lightheadedness, nausea, vomiting, and a spinning sensation.\\n\\nThe fact that you're experiencing these symptoms after taking Panadol (which is an anti-inflammatory medication) and sleeping for a few hours suggests that the issue might not be related to the medication itself. Instead, it could be related to the body's response to the medication.\\n\\nThe fact that the spinning sensation is more pronounced when you're lying down and less pronounced when you're sitting down might indicate that the body is more sensitive to changes in position when you're lying down. This could be due to the way the body's blood vessels respond to gravity when you're lying down.\\n\\nThe nausea and vomiting you're experiencing might be related to the medication or the body's response to the change in position. Some people with orthostatic intolerance may experience nausea and vomiting when they change positions, especially if they're not used to it.\\n\\nIt's also possible that you're experiencing a condition called Benign Paroxysmal Positional Vertigo (BPPV), which is a condition that causes brief, intense episodes of vertigo (dizziness) when you change positions. However, BPPV is usually triggered by changes in position, not just standing.\\n\\nThe fact that you're experiencing these symptoms at night and waking up with a spinning sensation might indicate that you're experiencing some degree of orthostatic intolerance, but it's not necessarily related to a medical condition.\\n\\nTo manage your symptoms, you can try the following:\\n\\n* Stay hydrated by drinking plenty of water throughout the day.\\n* Take breaks to sit or lie down when you need to change positions.\\n* Avoid standing for long periods of time, especially if you're not used to it.\\n* Try to relax and avoid stress, which can exacerbate orthostatic intolerance.\\n* Consider talking to your doctor about your symptoms, as they can help you rule out any underlying medical conditions.\\n\\nI hope this helps you understand what might be going on. If your symptoms persist or worsen, it's always best to consult with your doctor to rule out any underlying medical conditions.\"}]}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message = [ {\"role\": \"system\", \"content\": \"You are a health expert. Answer the user's question in detail.\"},\n",
        "            {\"role\": \"user\", \"content\": \"I woke up this morning feeling the whole room is spinning when i was sitting down. \"\n",
        "            \"I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. \"\n",
        "            \"After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, \"\n",
        "            \"only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, \"\n",
        "            \"the spinning lessen so i am not sure whether its connected or coincidences.\"}\n",
        "]\n",
        "\n",
        "result = pipeline_qn_peft(message, max_new_tokens=1000, do_sample=True, temperature=0.7)\n",
        "# Print the generated response\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def structure_output_from_result(result):\n",
        "    for output in result[0]['generated_text'] :\n",
        "        if output['role'] == 'assistant':\n",
        "            return output['content'].strip()\n",
        "\n",
        "def call_llm_experts(input_text, pipeline=pipeline_qn_peft):\n",
        "\n",
        "    message = [{\"role\": \"user\", \"content\": input_text}]\n",
        "    message = [ {\"role\": \"system\", \"content\": \"You are a health expert. Answer the user's question in detail.\"}] + message\n",
        "    print(\"Calling LLM with message:\", message)\n",
        "    result = pipeline(message, max_new_tokens=1000, do_sample=True, temperature=0.7)\n",
        "    return structure_output_from_result(result)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "* Running on public URL: https://9be5372e88907cf2ba.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://9be5372e88907cf2ba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling LLM with message: [{'role': 'system', 'content': \"You are a health expert. Answer the user's question in detail.\"}, {'role': 'user', 'content': 'i am feeling very happy always'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling LLM with message: [{'role': 'system', 'content': \"You are a health expert. Answer the user's question in detail.\"}, {'role': 'user', 'content': 'i feel more happiness when someone else is sad'}]\n"
          ]
        }
      ],
      "source": [
        "# create a gradio interface for the health expert model\n",
        "import gradio as gr \n",
        "def gradio_interface(question):\n",
        "    answer = call_llm_experts(question, pipeline_qn_peft)\n",
        "    return answer\n",
        "\n",
        "gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Textbox(label=\"Ask a health-related question\"),\n",
        "    outputs=gr.Text(label=\"Structured Answer\"),\n",
        "    title=\"Health Expert LLM\",\n",
        "    description=\"Ask a health-related question and get answers from the fine-tuned LLM.\"\n",
        ").launch(share=True)  # Set share=True to create a public link\n",
        "# Note: You can set share=False if you want to run it locally without a public link "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
