{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RVhYk5FZ_lb",
        "outputId": "6b061cbb-c80f-407c-cddb-b0c613024936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.55.0)\n",
            "Collecting peft\n",
            "  Using cached peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in ./.venv/lib/python3.11/site-packages (1.10.0)\n",
            "Collecting trl\n",
            "  Using cached trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from peft) (7.0.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.11/site-packages (from peft) (2.8.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.13.0->peft) (80.9.0)\n",
            "Collecting datasets>=3.0.0 (from trl)\n",
            "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=3.0.0->trl)\n",
            "  Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0.0->trl)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets>=3.0.0->trl)\n",
            "  Using cached pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Collecting xxhash (from datasets>=3.0.0->trl)\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Using cached aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets>=3.0.0->trl)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets>=3.0.0->trl)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
            "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, bitsandbytes, peft, datasets, trl\n",
            "\u001b[2K  Attempting uninstall: fsspecm\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [pyarrow]\n",
            "\u001b[2K    Found existing installation: fsspec 2025.7.05;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [pyarrow]\n",
            "\u001b[2K    Uninstalling fsspec-2025.7.0:;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [pyarrow]\n",
            "\u001b[2K      Successfully uninstalled fsspec-2025.7.08;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [pyarrow]\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [trl]237m━\u001b[0m \u001b[32m19/20\u001b[0m [trl]sets]tes]\n",
            "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 bitsandbytes-0.46.1 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.3 multiprocess-0.70.16 pandas-2.3.1 peft-0.17.0 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 trl-0.21.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers peft bitsandbytes accelerate trl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RaNkZNI7b3MF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLHnBie5fgiD",
        "outputId": "5c41d6cd-3ea6-47cc-fb03-e782526261a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "# use the pipeline for question - answer\n",
        "pipeline_qn = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70fBRBFPhnYJ",
        "outputId": "51d1c0e9-845a-4632-90fc-6990dd6e912c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.Your comments are  helpful. I am glad I am not the only one who has experienced this. I am also worried about my future health. I hope I am going to be okay. Please do the same thing and let me know if you feel the same. I feel very confused.'}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use qa_pipeline\n",
        "context = \"You are a doctor\"\n",
        "question = \"I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.Your comments are \"\n",
        "\n",
        "pipeline_qn(text_inputs=question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q28W-TiVjni2"
      },
      "outputs": [],
      "source": [
        "# Example: asking the model to summarize a text\n",
        "text_to_summarize = \"The quick brown fox jumps over the lazy dog. The fox is known for its speed, while the dog is known for its laziness. This simple sentence is often used to demonstrate a variety of linguistic concepts.\"\n",
        "\n",
        "# The prompt must follow the model's expected format\n",
        "prompt = f\"[INST]You are a doctor. Summarize the following text: {question}[/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZskANuejpP2",
        "outputId": "d0b7cc8e-5d47-416c-f143-2baee278c48a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm not sure what to make of your description. It sounds like you're experiencing symptoms of vertigo, which can be caused by a variety of factors, including inner ear problems, medication side effects, or even motion sickness.\n",
            "\n",
            "Vertigo is a condition where you feel like you're spinning or feeling off-balance, even when you're not actually moving around. This can be quite disorienting and uncomfortable. The good news is that vertigo is usually treatable with some simple lifestyle changes, such as avoiding triggers, taking medication, or practicing relaxation techniques.\n",
            "\n",
            "Here are some possible explanations for your symptoms:\n",
            "\n",
            "1. **Inner ear problems**: You may have an inner ear disorder, such as benign paroxysmal positional vertigo (BPPV), labyrinthitis, or vestibular neuritis, which can cause vertigo.\n",
            "2. **Medication side effects**: Certain medications, such as antibiotics, antihistamines, or antidepressants, can cause vertigo as a side effect.\n",
            "3. **Motion sickness**: You may be experiencing motion sickness, which is a common condition that can cause dizziness and nausea.\n",
            "4. **Other medical conditions**: Other conditions, such as high blood pressure, migraines, or Meniere's disease, can also cause vertigo.\n",
            "\n",
            "It's not normal stomach discomfort, and if you're experiencing vertigo, it's essential to consult with a healthcare professional to rule out any underlying medical conditions. They can help determine the cause of your symptoms and provide guidance on the best course of treatment.\n",
            "\n",
            "In the meantime, here are some tips that may help alleviate your symptoms:\n",
            "\n",
            "* **Take it easy**: Avoid strenuous activities and try to rest as much as possible.\n",
            "* **Stay hydrated**: Drink plenty of water to help your body recover.\n",
            "* **Avoid triggers**: If you're experiencing motion sickness, try to avoid triggers such as heavy lifting, bending, or traveling by car.\n",
            "* **Practice relaxation techniques**: Try relaxation techniques, such as deep breathing, meditation, or yoga, to help manage your symptoms.\n",
            "\n",
            "Remember, vertigo is usually a temporary condition, and with some simple lifestyle changes and medical attention, you can manage your symptoms and feel better soon.\n"
          ]
        }
      ],
      "source": [
        "# Run the pipeline with the formatted prompt\n",
        "result = pipeline_qn(prompt, max_new_tokens=500, do_sample=True, temperature=0.7)\n",
        "\n",
        "# The result is a list of dictionaries, so you'll need to extract the generated text\n",
        "generated_text = result[0]['generated_text']\n",
        "\n",
        "# remove all the texts between [INST] and [/INST] from the generated text. Use string index to Find [INST] and [/INST] and remove all the caharacters in between these including [INST] abd [/INST]\n",
        "start_index = generated_text.find(\"[INST]\")\n",
        "end_index = generated_text.find(\"[/INST]\")\n",
        "print(generated_text[end_index+7:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AhFO3QjLowum"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IkBuQbxlpbeK"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "\n",
        "# def format_data(example):\n",
        "#     context = f\"{example['instruction']} {example['input']}\\n doctor: {example['output']}\"\n",
        "#     return tokenizer(context, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# formatted_data = ds.map(format_data, remove_columns=ds[\"train\"].column_names)\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Example formatting function\n",
        "def format_data_for_llama(row):\n",
        "    # This is a common template for Llama instruct models.\n",
        "    # The 'input' field is optional and can be an empty string if not provided.\n",
        "    prompt = f\"[INST] {row['instruction']}\\n{row['input']} [/INST]\"\n",
        "    response = f\"{row['output']}\"\n",
        "    # Combine the prompt and response for the model to learn the full sequence.\n",
        "    return {\"text\": prompt + response}\n",
        "\n",
        "# Apply the formatting to the entire dataset\n",
        "formatted_dataset = ds.map(format_data_for_llama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDZabxoJyKq3",
        "outputId": "2638712d-e242-4018-8d3f-f0c0ad9533d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'text'],\n",
              "    num_rows: 112165\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tfZnZptHr7uN"
      },
      "outputs": [],
      "source": [
        "# Set up quantization config for 4-bit loading (saves GPU memory)\n",
        "# This is crucial for running larger models on limited hardware.\n",
        "# from bitsandbytes import BitsAndBytesConfig\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  \n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
        "    bnb_4bit_use_double_quant=True,  # ✅ Enable double quant for more VRAM savings\n",
        ")\n",
        "\n",
        "\n",
        "# LoRA configuration (Parameter-Efficient Fine-Tuning)\n",
        "# LoRA injects small, trainable matrices into the model, making fine-tuning\n",
        "# much more efficient than updating all model parameters.\n",
        "lora_config = LoraConfig(\n",
        "    r=4, # Rank of the update matrices (adjust based on experimentation)\n",
        "    lora_alpha=16, # Scaling factor for LoRA\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # Modules to apply LoRA to\n",
        "    lora_dropout=0.05, # Dropout probability for LoRA layers\n",
        "    bias=\"none\", # Do not train bias terms\n",
        "    task_type=\"CAUSAL_LM\", # Specify the task type\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "# These control the training process, including learning rate, epochs, etc.\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=1,         # minimal batch\n",
        "    gradient_accumulation_steps=32,        # reduce from 64 if you want faster step time\n",
        "    warmup_steps=2,\n",
        "    max_steps=50,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=200,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    dataloader_num_workers=2,\n",
        "    gradient_checkpointing=True,           # << reduces VRAM usage\n",
        ")\n",
        "\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     per_device_train_batch_size=2,\n",
        "#     gradient_accumulation_steps=4,\n",
        "#     learning_rate=2e-4,\n",
        "#     max_grad_norm=0.3,\n",
        "#     max_steps=500,\n",
        "#     logging_steps=10,\n",
        "#     fp16=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aySPY3tcu8Sr",
        "outputId": "58dc7de3-3d29-47c9-d5f4-e7f713f90b0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'text'],\n",
              "        num_rows: 112165\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reduce the dataset to 1000 samples for faster training\n",
        "formatted_dataset = formatted_dataset['train'].shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'text'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AfbOG5du0l3s"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'meta-llama/Llama-3.2-1B-Instruct',\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Automatically maps model to available devices (GPU/CPU)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size after quantization: 262.74 million parameters\n"
          ]
        }
      ],
      "source": [
        "# what is the model size after quantization\n",
        "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model size after quantization: {model_size / 1e6:.2f} million parameters\")  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "22KDDBIm0dM1"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "KNmoPEy5uqQR",
        "outputId": "fdc1293c-7876-4dd4-cffd-8488bdf20456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing SFTTrainer...\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing SFTTrainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=formatted_dataset,\n",
        "    peft_config=lora_config,\n",
        "    args=training_arguments\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4n45_0HD2iey"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"health_assistant_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "# load the model from the saved directory\n",
        "model = AutoModelForCausalLM.from_pretrained(\"health_assistant_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "# Example usage of the fine-tuned model\n",
        "pipeline_qn = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# wrap the above code in a function\n",
        "def ask_health_expert(question,pipeline):    \n",
        "    prompt = f\"[INST]You are a doctor. Generate answer to my question.My question: {question} . Your answer is : [/INST]\"\n",
        "    result = pipeline(prompt, max_new_tokens=500, do_sample=True, temperature=0.7)\n",
        "    generated_text = result[0]['generated_text']\n",
        "    start_index = generated_text.find(\"[INST]\")\n",
        "    end_index = generated_text.find(\"[/INST]\")\n",
        "    generated_txt=generated_text[end_index+7:]  # Return the answer after the [/INST] tag\n",
        "\n",
        "    # Remove any [INST] or [/INST] od \"[\" , \"]\"\n",
        "    generated_txt = generated_txt.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return generated_txt\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 5] Input/output error: '/home/sudip/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/refs/main'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:972\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid repo type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Accepted repo types are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(constants.REPO_TYPES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m hf_headers = \u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[39m, in \u001b[36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py:126\u001b[39m, in \u001b[36mbuild_hf_headers\u001b[39m\u001b[34m(token, library_name, library_version, user_agent, headers, is_write_action)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Get auth token to send\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m token_to_send = \u001b[43mget_token_to_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Combine headers\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py:154\u001b[39m, in \u001b[36mget_token_to_send\u001b[39m\u001b[34m(token)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Token is not provided: we get it from local cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m cached_token = \u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Case token is explicitly required\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_auth.py:49\u001b[39m, in \u001b[36mget_token\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03mGet token if user is logged in.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m \u001b[33;03m    `str` or `None`: The token, `None` if it doesn't exist.\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_token_from_google_colab() \u001b[38;5;129;01mor\u001b[39;00m _get_token_from_environment() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_get_token_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_auth.py:123\u001b[39m, in \u001b[36m_get_token_from_file\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _clean_token(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_TOKEN_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1059\u001b[39m, in \u001b[36mPath.read_text\u001b[39m\u001b[34m(self, encoding, errors)\u001b[39m\n\u001b[32m   1058\u001b[39m encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1045\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1044\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m io.open(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
            "\u001b[31mOSError\u001b[39m: [Errno 5] Input/output error: '/home/sudip/.cache/huggingface/token'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load base model\u001b[39;00m\n\u001b[32m      6\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-1B-Instruct\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# e.g., \"meta-llama/Llama-2-7b-hf\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m base_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[32m     10\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:508\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[32m    507\u001b[39m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m         resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m         commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    264\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    265\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    266\u001b[39m     **kwargs,\n\u001b[32m    267\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:531\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m resolved_files = \u001b[43m[\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfull_filenames\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:532\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    531\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    534\u001b[39m ]\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:144\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    137\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    138\u001b[39m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m ):\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     resolved_file = \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1390\u001b[39m, in \u001b[36mtry_to_load_from_cache\u001b[39m\u001b[34m(repo_id, filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m   1388\u001b[39m     revision_file = os.path.join(refs_dir, revision)\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(revision_file):\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(revision_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1391\u001b[39m             revision = f.read()\n\u001b[32m   1393\u001b[39m \u001b[38;5;66;03m# Check if file is cached as \"no_exist\"\u001b[39;00m\n",
            "\u001b[31mOSError\u001b[39m: [Errno 5] Input/output error: '/home/sudip/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/refs/main'"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load base model\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # e.g., \"meta-llama/Llama-2-7b-hf\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load PEFT adapter\n",
        "peft_model_id = \".model/health_assistant_model\"  # path where you saved with save_model()\n",
        "model_peft = PeftModel.from_pretrained(base_model, peft_model_id)\n",
        "\n",
        "# Merge adapter into base model (optional but can improve performance)\n",
        "# model = model.merge_and_unload()\n",
        "\n",
        "# Set to evaluation mode\n",
        "model_peft.eval()\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_peft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipeline_qn_peft = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_peft,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! Thanks for your question\n",
            "It sounds like you're experiencing a couple of different symptoms at the same time\n",
            "The spinning sensation when you're sitting down is likely due to the fact that you're sitting upright, which can make you feel dizzy\n",
            "The nausea and vomiting are probably related to the stomach discomfort you're feeling\n",
            "The fact that you're experiencing a sense of spinning when you're trying to move around suggests that you may be experiencing a condition called orthostatic intolerance, which is a type of autonomic nervous system disorder\n",
            "It's a common condition in people with anxiety or other underlying medical conditions\n",
            "The fact that your symptoms are related to your stomach discomfort suggests that your autonomic nervous system is being affected, which may be due to an underlying medical condition such as gastroparesis or another gastrointestinal disorder\n",
            "It's unlikely that your symptoms are coincidental and there may be a medical cause for your symptoms\n",
            "I would recommend that you see your doctor for further evaluation and treatment.\n"
          ]
        }
      ],
      "source": [
        "result = ask_health_expert(question,pipeline_qn_peft)\n",
        "\n",
        "# structure the long text output from the model\n",
        "def structure_output(text):\n",
        "    # Split the text into sentences or paragraphs\n",
        "    sentences = text.split('. ')\n",
        "    structured_output = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        # Clean up each sentence\n",
        "        cleaned_sentence = sentence.strip()\n",
        "        if cleaned_sentence:  # Only add non-empty sentences\n",
        "            structured_output.append(cleaned_sentence)\n",
        "    \n",
        "    return structured_output\n",
        "\n",
        "structured_result_1 = structure_output(result)\n",
        "\n",
        "for idx, sentence in enumerate(structured_result_1, 1):\n",
        "    print(f\"{sentence}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "* Running on public URL: https://d73a38b05f42cb1223.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://d73a38b05f42cb1223.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# create a gradio interface for the health expert model\n",
        "import gradio as gr \n",
        "def gradio_interface(question):\n",
        "    answer = ask_health_expert(question, pipeline_qn_peft)\n",
        "    structured_answer = structure_output(answer)\n",
        "    return structured_answer\n",
        "\n",
        "gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Textbox(label=\"Ask a health-related question\"),\n",
        "    outputs=gr.Text(label=\"Structured Answer\"),\n",
        "    title=\"Health Expert LLM\",\n",
        "    description=\"Ask a health-related question and get answers from the fine-tuned LLM.\"\n",
        ").launch(share=True)  # Set share=True to create a public link\n",
        "# Note: You can set share=False if you want to run it locally without a public link \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'system',\n",
              "    'content': \"You are a health expert. Answer the user's question in detail.\"},\n",
              "   {'role': 'user',\n",
              "    'content': 'I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.'},\n",
              "   {'role': 'assistant',\n",
              "    'content': \"I can understand your concerns and symptoms you are describing. The feeling of spinning or vertigo when standing still, walking, or even just sitting down is called orthostatic vertigo. It's a common condition that can be caused by a variety of factors, including inner ear problems, low blood pressure, and medication side effects.\\n\\nThe nausea and vomiting you experienced, especially after taking Panadol (acetaminophen), suggest that your inner ear is not functioning properly. This can lead to a condition called Meniere's disease, which is a disorder of the inner ear that affects balance, hearing, and equilibrium.\\n\\nThe fact that your spinning sensation is only present when you want to move around suggests that your inner ear is not functioning properly. This could be due to a problem with the vestibular system, which is responsible for balance and spatial orientation.\\n\\nThere are a few possible explanations for your symptoms:\\n\\n1. **Inner ear problems**: As mentioned earlier, your inner ear may be affected by Meniere's disease, which can cause vertigo, tinnitus (ringing in the ears), and hearing loss.\\n2. **Medication side effects**: Panadol can cause dizziness or vertigo as a side effect, especially if you take it with other medications that can affect the inner ear.\\n3. **Low blood pressure**: Hypotension (low blood pressure) can cause a feeling of spinning or vertigo, especially when you're standing still.\\n4. **Anxiety or stress**: Stress and anxiety can cause a feeling of spinning or vertigo, especially if you're experiencing a panic attack or are under a lot of pressure.\\n\\nIt's possible that your symptoms are coincidental, and it's not necessarily related to the medication you took. However, it's always a good idea to consult with a healthcare professional to rule out any underlying medical conditions.\\n\\nIn the meantime, here are some suggestions that may help:\\n\\n1. **Stay hydrated**: Drink plenty of water to help your body regulate its fluid balance.\\n2. **Get plenty of rest**: Try to get plenty of rest and avoid stimulating activities that can make your symptoms worse.\\n3. **Avoid standing still for long periods**: Try to move around and stretch every few hours to help alleviate your symptoms.\\n4. **Avoid heavy lifting or bending**: If you have a job that requires heavy lifting or bending, try to take regular breaks to avoid exacerbating your symptoms.\\n\\nIf your symptoms worsen or persist, it's always best to consult with a healthcare professional for further evaluation and guidance.\"}]}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message = [ {\"role\": \"system\", \"content\": \"You are a health expert. Answer the user's question in detail.\"},\n",
        "            {\"role\": \"user\", \"content\": \"I woke up this morning feeling the whole room is spinning when i was sitting down. \"\n",
        "            \"I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. \"\n",
        "            \"After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, \"\n",
        "            \"only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, \"\n",
        "            \"the spinning lessen so i am not sure whether its connected or coincidences.\"}\n",
        "]\n",
        "\n",
        "result = pipeline_qn_peft(message, max_new_tokens=1000, do_sample=True, temperature=0.7)\n",
        "# Print the generated response\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def structure_output_from_result(result):\n",
        "    for output in result[0]['generated_text'] :\n",
        "        if output['role'] == 'assistant':\n",
        "            return output['content'].strip()\n",
        "\n",
        "def call_llm_experts(input_text, pipeline=pipeline_qn_peft):\n",
        "\n",
        "    message = [{\"role\": \"user\", \"content\": input_text}]\n",
        "    message = [ {\"role\": \"system\", \"content\": \"You are a health expert. Answer the user's question in detail.\"}] + message\n",
        "    print(\"Calling LLM with message:\", message)\n",
        "    result = pipeline(message, max_new_tokens=1000, do_sample=True, temperature=0.7)\n",
        "    return structure_output_from_result(result)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7865\n",
            "* Running on public URL: https://4b3a46e6a178eb15ed.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4b3a46e6a178eb15ed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling LLM with message: [{'role': 'system', 'content': \"You are a health expert. Answer the user's question in detail.\"}, {'role': 'user', 'content': 'medicne to reduce diabetes'}]\n",
            "Calling LLM with message: [{'role': 'system', 'content': \"You are a health expert. Answer the user's question in detail.\"}, {'role': 'user', 'content': 'my head is spinning'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 626, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 2250, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1757, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 917, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_8285/963848640.py\", line 4, in gradio_interface\n",
            "    answer = call_llm_experts(question, pipeline_qn_peft)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_8285/1271535839.py\", line 11, in call_llm_experts\n",
            "    result = pipeline(message, max_new_tokens=1000, do_sample=True, temperature=0.7)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py\", line 314, in __call__\n",
            "    return super().__call__(Chat(text_inputs), **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1458, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1465, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1364, in forward\n",
            "    model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1268, in _ensure_tensor_on_device\n",
            "    return UserDict({name: self._ensure_tensor_on_device(tensor, device) for name, tensor in inputs.items()})\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1268, in <dictcomp>\n",
            "    return UserDict({name: self._ensure_tensor_on_device(tensor, device) for name, tensor in inputs.items()})\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/media/sudip/linux-extra/code/llm-fine-tune/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1274, in _ensure_tensor_on_device\n",
            "    return inputs.to(device)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "torch.AcceleratorError: CUDA error: unspecified launch failure\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create a gradio interface for the health expert model\n",
        "import gradio as gr \n",
        "def gradio_interface(question):\n",
        "    answer = call_llm_experts(question, pipeline_qn_peft)\n",
        "    return answer\n",
        "\n",
        "gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Textbox(label=\"Ask a health-related question\"),\n",
        "    outputs=gr.Text(label=\"Structured Answer\"),\n",
        "    title=\"Health Expert LLM\",\n",
        "    description=\"Ask a health-related question and get answers from the fine-tuned LLM.\"\n",
        ").launch(share=True)  # Set share=True to create a public link\n",
        "# Note: You can set share=False if you want to run it locally without a public link "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
